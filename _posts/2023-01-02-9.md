---
title: "Black Box Problem과 XAI (feat. GradCAM)"
classes: wide
toc: false
categories:
  - data-ai
---

관련 소스 링크: [https://github.com/juhonamnam/data-analysis/blob/main/xai/xai.ipynb](https://github.com/juhonamnam/data-analysis/blob/main/xai/xai.ipynb)

<!--excerpt open-->

저는 인공지능을 통해 코딩을 시작하게 되었습니다. 수학전공자로 인공지능 수학 수업을 들으며 Python을 사용하게 되었고, 본격적으로 개발자의 길을 가기로 결심하게 되었기 때문입니다. 그중에서도 XAI는 수업에서 발표주제로 삼았던 내용이기도 합니다. 이번 포스팅에서는 당시 발표했던 내용을 공유합니다.

<!--excerpt close-->

## 통계 분석 vs 인공지능

통계 분석과 인공지능의 가장 큰 차이는 모델 도출 방식입니다. 통계 분석은 이론에 근거하여 모델을 도출하고 인공지능은 데이터 학습을 통해 모델을 도출합니다. 물론 인공지능 또한 이론적인 부분을 고려하긴 합니다만, 가중치를 학습하는데 있어서 데이터의 패턴이 훨씬 주요한 요소로 작용합니다.

데이터에 의해 높은 정확도로 학습된 모델을 바로 실생활에 적용하는 데에는 여러 우려사항이 있습니다. 인과관계를 무시하고 상관관계만을 고려하는 오류를 범할 수 있기 때문이죠. 따라서 아무리 정확도가 높은 인공지능 모델이라 할지라도 내부 처리 방식을 분석할 필요가 있습니다.

## Black Box Problem

<figure>
  <img src="/images/9/1.png" alt="image1">
  <figcaption>출처: https://www.investopedia.com/terms/b/blackbox.asp</figcaption>
</figure>

Black Box 모델이란 입력과 출력 및 수행 기능을 알려져 있으나 내부 처리 방식은 알려져 있지 않은 모델을 말합니다. 인공지능 모델의 경우 데이터를 통해 파라미터를 학습하기에 인과관계보다 상관관계가 우선시 됩니다. 따라서 높은 정확도를 보이더라도 내부 처리 방식에서 잘못된 인과관계가 학습될 수 있습니다.

<figure>
  <img src="/images/9/neural-network-diagram.svg" alt="neural-network-diagram">
  <figcaption>출처: https://www.tibco.com/reference-center/what-is-a-neural-network</figcaption>
</figure>

인공지능 모델에 있어서 내부 처리 방식은 학습된 파라미터들의 의미를 파악함으로써 이해할 수 있습니다. 일차함수와 같이 파라미터의 의미가 확실한 경우 어렵지 않지만 위와 같은 neural network 모델에서 각각의 파라미터가 같는 의미를 일일이 파악하는 것은 사람의 능력으로는 불가능에 가깝습니다.

<figure>
  <img src="/images/9/2.png" alt="image2">
  <img src="/images/9/3.png" alt="image3">
  <figcaption>출처: https://theblue.ai/blog/lime-models-explanation/</figcaption>
</figure>

위의 사진은 허스키와 늑대를 구분하는 인공지능입니다. 보시다시피 하나의 경우만 제외하고 전부 잘 예측을 하고있어서 정확도가 높은 모델이라고 할 수 있습니다.

<figure>
  <img src="/images/9/4.png" alt="image4">
  <figcaption>출처: https://theblue.ai/blog/lime-models-explanation/</figcaption>
</figure>

그러나 실제 이 모델의 판단 근거를 Lime 알고리즘으로 시각화해서 나타내면 위와 같습니다. 동물의 특징보다는 배경을 보고 판단한 흔적들이 보이죠. 결코 좋은 모델이라고 볼 수 없습니다.

## Black Box 모델의 잠재적 위험성

사람의 능력으로 이해하기 어려운 인공지능의 판단에 의존하는 것은 큰 위험이 따를 수 있습니다.

<figure>
  <img src="/images/9/5.jpg" alt="image5">
  <figcaption>사고 당시 ABC 뉴스 보도화면 (2019.11.9)</figcaption>
</figure>

2018년도에는 우버 자율주행 시험 중 보행자를 감지하지 못하여 사망에 이르게 한 사건이 있었습니다. 물론 해당 사건은 작업자가 주행 중 시스템을 관찰하지 않고 핸드폰을 보는 행위를 하여 직무 태만으로 결론이 났지만, 보행자를 보고 멈추지 않았던 인공지능의 판단 또한 분석할 필요가 있었을 것입니다.

<figure>
  <img src="/images/9/6.png" alt="image6">
  <figcaption>출처: Explaining and Harnessing Adversarial Examples [ICLR 2015]</figcaption>
</figure>

인공지능의 내부 처리 방식을 이용한 공격 또한 가능하죠. 사람의 눈으로는 구분되지 않는 노이즈를 이미지에 더해 인공지능의 판단을 완전히 바꿀 수 있습니다. 이러한 방식으로 생성된 이미지를 **적대적 예제**라고 합니다.

자율주행, 의학 진단, 신용등급 결정, 채용 등 여러 분야에서 인공지능 모델이 도입되고 있습니다. 삶에 직접적인 영향을 주는 판단을 취약점이 가득한 인공지능 모델에 의존하는 것은 상당이 위험할 수 있겠죠. 따라서 인공지능의 판단에 대한 분석이 필요합니다.

## Explainable Artificial Intelligence (XAI)

XAI는 AI가 내린 결정을 사람이 이해하는 형태로 설명하고 제시할 수 있는 기술을 말합니다. 데이터의 각 속성에 대해서 AI의 결정에 기여하는 정도를 수치화하여 시각화하는 것이 일반적인 방법입니다.

<figure>
  <img src="/images/9/7.png" alt="image7">
  <figcaption>출처: https://www.researchgate.net/figure/The-trade-off-between-interpretability-and-accuracy-of-some-relevant-ML-models-Highly_fig4_335937022</figcaption>
</figure>

인공지능의 판단을 분석하는 데에는 데이터와 모델의 유형에 따라 여러 기법들이 존재합니다. 일반적으로 정확도가 높은 모델일수록 해석력은 떨어집니다.

오늘은 XAI의 여러 예시 중 GradCAM을 살펴보겠습니다.

### GradCAM

GradCAM은 이미지 데이터의 대표적인 인공신경망 모델인 Convolutional Neural Network (CNN)의 판단 근거를 시각화하기 위한 모델입니다.

<figure>
  <img src="/images/9/8.png" alt="image8">
  <figcaption>출처: https://tyami.github.io/deep%20learning/CNN-visualization-Grad-CAM/</figcaption>
</figure>

CNN에는 이미지의 특징을 살려내기 위한 convolutional layer가 있습니다. 각 convolutional layer에서는 filer와 합성곱을 통해 feature map이 생성됩니다. GradCAM 기법에서는 가장 마지막 convolutional layer를 통해 생성된 feature map을 사용합니다.

마지막 convolutional layer의 feature map에서 각 픽셀의 gradient를 구하게 되는데 여기서 gradient는 특정 class에 특정 input이 주는 영향력을 말하고, 역전파를 통해 구해집니다. 각 feature map에서 픽셀별 평균 gradient를 곱하여 heatmap을 만들고, 모든 feature map에 ReLU 함수를 적용 후 (양의 가중치만 골라내기 위해) 더하게 되면 GradCAM이 됩니다.

<figure>
  <img src="/images/9/9.png" alt="image9">
</figure>

저는 GradCAM을 코드로 구현해보기 위해 Tensorflow 2를 기반으로 하는 `tf-explain`이라는 라이브러리를 사용했습니다. `tf-explain`은 Sicara라고 하는 프랑스 파리의 한 기업에서 개발된 라이브러리로 GradCAM을 포함한 여러 이미지 기반 XAI 기법을 제공합니다. 저는 VGG16 모델의 예측에 대해 분석하였습니다.